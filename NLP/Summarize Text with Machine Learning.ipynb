{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a90ec285",
   "metadata": {},
   "source": [
    "Text Summarization involves condensing a piece of text into a shorter version, reducing the size of the original text while preserving key information and the meaning of the content. Since manual text synthesis is a long and generally laborious task, task automation is gaining in popularity and therefore a strong motivation for academic research. In this article, I will take you through the task of Natural Language Processing to summarize text with Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de06b6c",
   "metadata": {},
   "source": [
    "In Machine Learning, there are important applications for text summarization in various Natural Language Processing related tasks such as text classification, answering questions, legal text synthesis, news synthesis, and headline generation which can be achieved with Machine Learning. The intention to summarize a text is to create an accurate and fluid summary containing only the main points described in the document.\n",
    "\n",
    "#### Types of Approaches to Summarize Text\n",
    "\n",
    "Generally, Text Summarization is classified into two main types: Extraction Approach and Abstraction Approach. Now let’s go through both these approaches before we dive into the coding part.\n",
    "\n",
    "#### 1. The Extractive Approach\n",
    "\n",
    "The Extractive approach takes sentences directly from the document according to a scoring function to form a cohesive summary. This method works by identifying the important sections of the text cropping and assembling parts of the content to produce a condensed version.\n",
    "\n",
    "#### 2. The Abstractive Approach\n",
    "\n",
    "The Abstraction approach aims to produce a summary by interpreting the text using advanced natural language techniques to generate a new, shorter text – parts of which may not appear in the original document, which conveys the most information.\n",
    "\n",
    "Here extractive approach is used  to summarize text using Machine Learning and Python. Will be using the TextRank algorithm which is an extractive and unsupervised machine learning algorithm for text summarization.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd47615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "df = pd.read_csv(\"tennis.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c68167",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['article_text'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c585fe2",
   "metadata": {},
   "source": [
    " split the sequences into the data by tokenizing them using a list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7586a12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentences = []\n",
    "for s in df['article_text']:\n",
    "    sentences.append(sent_tokenize(s))\n",
    "\n",
    "sentences = [y for x in sentences for y in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918a924c",
   "metadata": {},
   "source": [
    "Now I am going to use the Glove method for word representation, it is an unsupervised learning algorithm developed by Stanford University to generate word integrations by aggregating the global word-to-word co-occurrence matrix from a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e848eff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()\n",
    "\n",
    "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "clean_sentences = [s.lower() for s in clean_sentences]\n",
    "stop_words = stopwords.words('english')\n",
    "def remove_stopwords(sen):\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new\n",
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8913728",
   "metadata": {},
   "source": [
    "create vectors for the sentences:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25c664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "for i in clean_sentences:\n",
    "  if len(i) != 0:\n",
    "    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "  else:\n",
    "    v = np.zeros((100,))\n",
    "  sentence_vectors.append(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad64042",
   "metadata": {},
   "source": [
    "Finding Similarities to Summarize Text\n",
    "\n",
    "The next step is to find similarities between the sentences, and I will use the cosine similarity approach for this task. Let’s create an empty similarity matrix for this task and fill it with cosine similarities of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cfef49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "for i in range(len(sentences)):\n",
    "  for j in range(len(sentences)):\n",
    "    if i != j:\n",
    "      sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb12a5d3",
   "metadata": {},
   "source": [
    "Now I am going to convert the sim_mat similarity matrix into the graph, the nodes in this graph will represent the sentences and the edges will represent the similarity scores between the sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5e62bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268065bc",
   "metadata": {},
   "source": [
    "let’s summarize text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5a93e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "for i in range(5):\n",
    "  print(\"ARTICLE:\")\n",
    "  print(df['article_text'][i])\n",
    "  print('\\n')\n",
    "  print(\"SUMMARY:\")\n",
    "  print(ranked_sentences[i][1])\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e4987b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fdf59a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e9171d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
